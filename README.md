# ğŸ“¢ Speech Insight Analyzer

> **Why?**
> This tool transforms **raw audio** into **emotion-aware transcripts** to help speakers, content creators, and educators analyze, reflect, and grow.

---

## âš¡ Introduction

**Speech Insight Analyzer** is a lightweight, modular Python tool designed to:

* ğŸ™ï¸ Transcribe `.wav` audio using **Vosk** (lightweight, offline speech-to-text engine)

* ğŸ˜ƒ Detect emotions at the word level using **HuggingFace Transformers**

* ğŸ“Š Assign an "impact score" to help evaluate public speaking delivery

* ğŸ¯ Save structured output for further processing (e.g., generating videos from audio)

Built initially on **18th Dec 2024**, this tool was optimized for fast, offline usage in Colab and local environments.

---

# ğŸ§­ Step One of a Larger Vision

This project is Step One of a more advanced speech insight system.

While this version is built in Python for fast prototyping and public sharing, a more powerful and extensible version is currently being developed privately in Java, designed to include:

ğŸ™ï¸ Real-time voice analysis

ğŸ§  Advanced emotional impact scoring

ğŸ“ˆ Progress tracking and feedback loop

ğŸ•º Integration of voice

---

## ğŸ“š Table of Contents

1. [Why Vosk?](#why-vosk)
2. [Installation](#installation)
3. [How It Works](#how-it-works)
4. [Usage](#usage)
5. [Folder Structure](#folder-structure)
6. [Output Examples](#output-examples)
7. [Limitations](#limitations)
8. [Future Work](#future-work)

---

## ğŸ”¥ Why Vosk?

Vosk is:

* âœ… **Lightweight** (runs on CPU, even on Raspberry Pi)

* ğŸš€ **Fast** transcription engine

* ğŸ“´ **Offline** â€” no internet needed once the model is downloaded

* ğŸ¯ Perfect for **WAV mono PCM audio** transcription

---

## ğŸ“‚ Try It Now

You can run the notebook directly in Google Colab:

[ğŸ‘‰ Click here to open Colab](https://colab.research.google.com/drive/11bVoELFBvSFEou-KfPiK4nmac1-rE53g?usp=sharing)

---

## âš™ï¸ Installation

### ğŸ”Œ Core Dependencies

```bash
pip install vosk
pip install soundfile
apt-get install -y ffmpeg
pip install transformers torch nltk requests
```

### ğŸ“¦ Download Vosk Model (Example for English)

```bash
mkdir -p /content/drive/MyDrive/vosk_models
wget https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip \
     -O /content/drive/MyDrive/vosk_models/vosk-model-small-en-us-0.15.zip
unzip /content/drive/MyDrive/vosk_models/vosk-model-small-en-us-0.15.zip \
      -d /content/drive/MyDrive/vosk_models/
```

### ğŸ“ Model Path

Update your code:

```python
model_path = "/content/drive/MyDrive/vosk_models/vosk-model-small-en-us-0.15"
```

---

## ğŸš€ How It Works

1. Converts `.mp3` or `.m4a` to `.wav` via `ffmpeg`

2. Transcribes `.wav` to text with timestamps using Vosk

3. Detects emotions using HuggingFace `distilroberta-base`

4. Scores word-level impact

5. Saves all output to `/output/` folder

---

## ğŸ§ª Usage

### ğŸ¯ Run from CLI:

```bash
python main.py --audio path/to/input.mp3 --model path/to/vosk-model
```

### ğŸ“ Output:

* `output/yourfile_plain.txt` â†’ plain transcription

* `output/yourfile_timestamps.txt` â†’ words with timestamps

* `output/yourfile_emotions.json` â†’ emotions + impact scores

---

## ğŸ—‚ï¸ Folder Structure

```
speech-insight-analyzer/
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ models/                  # (ignored in .gitignore)
â”œâ”€â”€ output/                  # auto-generated outputs
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ speech_processor.py  # Vosk-based transcription
â”‚   â”œâ”€â”€ emotion_analyzer.py  # HuggingFace emotion scoring
â”‚   â””â”€â”€ export_utils.py      # file-saving logic
â””â”€â”€ README.md
```

---

## ğŸ§¾ Output Examples

### ğŸ“„ Plain Transcription:

```
this is your final test audio input without any punctuation needed
```

### ğŸ• Timestamp Log:

```
this: [0.42s - 0.60s]
is: [0.61s - 0.75s]
your: [0.76s - 0.93s]
...
```

### ğŸ’¥ Emotions JSON:

```json
[
  {
    "word": "great",
    "start": 2.41,
    "end": 2.91,
    "impact_score": 0.76,
    "emotion": "joy"
  }
]
```

---

## âš ï¸ Limitations

* Only supports **mono WAV audio** for transcription

* No punctuation or sentence boundary detection

* One language per model (no multilingual support)

* Internet required for HuggingFace emotion model

---

## ğŸŒ± Future Work

* âœ… Java version with GUI (planned) and extended features

* âœ… Streamlit app for real-time speaking practice

* ğŸ”„ Add sentence boundary detection

* ğŸ“Š Dashboard for progress review and emotion tracking

---

## âœ¨ Author

Built by **\[Manjunata H]** with â¤ï¸

---

## ğŸ“§ Contact & Contributions

* ğŸ“© **Email:** [Reach out here](manjunathakoshinum@gmail.com)

* ğŸŒŸ If you found this useful, **please give it a star!**

* ğŸ¤ Want to improve it? Feel free to **submit a pull request** â€” though note:

  > âš ï¸ I am **not actively maintaining the Python version**, as the project is being further developed in **Java**.

* ğŸ’¼ [Connect on LinkedIn](https://www.linkedin.com/in/manjunathah)

---
